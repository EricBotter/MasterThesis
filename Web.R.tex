\documentclass[mscthesis]{usiinfthesis}
\usepackage{lipsum}
\usepackage{listings}

\lstdefinelanguage{algebra}
{morekeywords={import,sort,constructors,observers,transformers,axioms,if,
else,end},
sensitive=false,
morecomment=[l]{//s},
}

\title{A report on the decentralized web} %compulsory
\specialization{Specialization in Computer Systems}%optional
\subtitle{} %optional
\author{Eric Botter} %compulsory
\begin{committee}
\advisor{Prof.}{Fernando}{Pedone} %compulsory
\coadvisor{}{Leandro Pacheco}{De Sousa}{} %optional
\end{committee}
\Day{20} %compulsory
\Month{June} %compulsory
\Year{2018} %compulsory, put only the year
\place{Lugano} %compulsory

\dedication{} %optional
\openepigraph{Someone said \dots}{Someone} %optional

%\makeindex %optional, also comment out \theindex at the end

\begin{document}

\maketitle %generates the titlepage, this is FIXED

\frontmatter %generates the frontmatter, this is FIXED

\begin{abstract}
This is a very abstract abstract.

\lipsum
\end{abstract}

%\begin{abstract}[Zusammenfassung]
%optional, use only if your external advisor requires it in his/er language
%\end{abstract}

\begin{acknowledgements}
\lipsum
\end{acknowledgements}

\tableofcontents
%\listoffigures %optional
%\listoftables %optional

\mainmatter

\chapter{Introduction}\label{ch:intro}
%TODO: summary of what we're going to do


\chapter{Background: The Current Web, Models and Definitions}
\label{ch:background}

The World Wide Web is probably the most popular and used service of the Internet, sometimes even confused with the Internet itself. It is very common nowadays to access the WWW (or most commonly known as simply ``the Web'') and browse websites from many platforms, from the typical desktop computer to the modern smartphone.

Let us define the scenario in which the Web lives. It is based on a client-server architecture, where Web servers provide objects (such as documents, images or files in general) to clients that request and display them, called \textit{user agents} (e.g. Web browsers).

In the Web, documents and objects are identified by a Uniform Resource Locator (URL), whose most important component is the domain name: it is a human-readable label that identifies a device within the Internet.
A domain name is composed by sequences of letters and symbols, separated by each other with dots. This separation is needed by the hierarchical structure of domain names, but we won't delve into the details of the Domain Name System here. More information can be found in Appendix \ref{appx:dns}. %TODO - expand on DNS flaws?

To access a website, the client has to know the domain name associated to that website. This is usually provided by the user or by services such as search engines. The domain name is resolved to an IP address by using the Domain Name System.
Once obtained, the client opens a TCP connection towards that address on port 80, and starts exchanging messages using the HyperText Transfer Protocol (HTTP). %TODO - mention HTTPS

HTTP is a client-server, request-response protocol. Clients specify the details of the needed resource in the request and the server replies with the content or an error status if something went wrong (e.g. 404 Not Found). We won't explore HTTP as none of the projects that we will see rely on HTTP or any of its properties. %TODO explore HTTP? maybe expand on it as needed

There are different ways to setup a website. A content creator can either setup a custom server and upload a website there, or it can rent a server (either physical or virtual) from an existing provider.

\section{Problems in the current web} \label{sec:problems}
The main problem in the current Web is vulnerability to \textbf{censorship}. Since we have a direct relationship from domain names to websites (or from IP addresses to websites), it is relatively easy for powerful parties (including governments and ISPs) to block communications from users to a certain service. 
The main attacks that can be used to prevent communication towards a website are:
\begin{itemize}
	\item Denial of Service (DoS): a large volume of requests is sent towards the targeted server, which quickly runs out of available resources (such as bandwidth, simultaneously open connections, memory or CPU). Requests can be sent from a single device, but in current days requests are typically sent from multiple sources, in order to both increase the volume of traffic and make it difficult to identify and stop the origin of the attack: this is known as Distributed Denial of Service (DDoS).
	\item IP address blocking: packets towards a given address or address range are blocked. This attack can be enacted by routers that exchange packets regarding the targeted IP address, which can interrupt forwarding of said packets thus preventing any sort of communication, making the server effectively disconnected from the Internet.
	\item DNS hijacking: by altering DNS resolutions, the domain of the targeted website can either be deleted or edited to make it refer to another IP address, thus preventing access to the original content. This attack can be carried out by both the owners of the DNS resolver (by directly editing their records), or by third parties through an attack called DNS cache poisoning: an attacker pretending to be a valid name server intercepts DNS requests from other name servers and provides fake responses to alter the address of the targeted domain, also setting a high time-to-live so that the redirection is active for as long as possible. Another vector for DNS hijacking, though unrelated to DNS itself, is to remotely edit the configuration of typical home routers through known vulnerabilities, changing the DNS resolver to a malicious one.
\end{itemize}

We also have a problem of \textbf{trust}. When you access a website, there is no guarantee that the data you received is from the content creator, because HTTP is vulnerable to man-in-the-middle attacks. There is no mechanism to verify the authenticity of the transmitted data and the protocol does not use encryption, so anyone can forge a valid HTTP communication (even based on an ongoing one) and send it through the wires. We expand on this in Section \ref{sec:distributed}.

HTTPS resolves this issue by asymmetrically encrypting the communication channel, and authenticating the data that is sent, but the current trust system (X.509) is based on certificate authorities and is considered weak, which might allow for identity theft. %TODO - expand on this

Another important issue is \textbf{privacy} and handling of personal information: with the current scenario, whenever you connect to a website, that website privately stores data about you.
This data can be either automatically collected from user interactions, or can be provided directly by the user: consider, as an example, a social network, where users provide personal information such as their generalities, and the website collects data such as post interactions, number and timestamps of logins, and so on.
This effectively moves ownership of the data from the user to the company. Data that intrinsically belongs to the user (especially personal information such as name, address and phone number) are stored privately into the company server, and the user has limited control over it, since the only possible actions on the data are the ones defined by the company or required by law.

\section{A Distributed Environment} \label{sec:distributed}
We have to rethink the Web if we want to move it to a decentralized environment. The current Web is a centralized system: each website is owned by a party that we'll define as \textit{content creator}. The content creator owns the website and is responsible for distributing its content, either by using a self-owned and maintained web server or by publishing it to a dedicated service, known as \textit{web hosting} service provider (there are too many services currently online to present a somewhat accurate list of examples here). When using \textit{web server}, we will always refer to both these options, since in both cases there is always a web server that serves the website, whether it's owned by the content provider or by a company.
Although it's not required, the content creator usually also obtains a domain name to associate with the website.

This system is centralized because the website is accessible only through the web server. If obtained, the domain name will always direct towards that server (even if it changes its IP address, since that's one of the main purposes of DNS).

Let us introduce a very important concept in distributed systems: \textbf{failure}, and its related models.
We introduce it now to highlight the difference between a centralized environment and a decentralized (or distributed) one.
The failure model in which we could place the Web is a \textbf{stopping failure model}: an entity can fail only by stopping, or \textit{crashing}. This means that we \textit{trust} each entity to behave as expected by the protocol, or to not function at all.\footnote{This is an oversimplification of the model, but we don't need to give more detail than this.} %TODO - expand on this? or point to a reference? We might move the technical details into the Appendix, and keep the discussion informal here.

This model allows Web clients (browsers) %TODO - I haven't defined them
and content creators to make certain assumptions on the behavior of web servers and other components:
\begin{itemize}
	\item Web clients assume that the content they request is returned without any modification, with its content exactly as intended by its creator;
	\item It is assumed that the channel through which the information is sent does not modify that data;
	\item Content creators assume that the website that they create is stored in the web server (and distributed) without any modification.
	%TODO - more?
\end{itemize}

One could argue that this model does not accurately represent the reality. For example, if a malicious third party takes over the communication channel, it can interfere %TODO - should we explain how? the following footnote is more technical than the tone used here
and change the data that is transmitted. This is transparent to both the clients and the web server, since there is no mechanism in place to ensure that the data is \textit{integral}\footnote{We mean a stronger integrity than the one provided by TCP: it ensures data integrity against transmission errors, which are in the order of few bits per kilobyte; but in this attack the entire TCP packet can be rewritten, allowing them to appear unaltered.} -- and rightfully so, given that the communication channel is assumed to not alter data arbitrarily. This is known as a \textit{man-in-the-middle} attack.

To allow this scenario, we need to weaken the model, by removing assumptions on what the communication channel will do, especially during a failure.
The \textbf{Byzantine failure model} is exactly this: entities can fail not only by stopping, but also by exhibiting arbitrary behavior.
This is particularly useful to model malicious intentions: since failed entities can perform arbitrary actions, it is particularly important to consider situations that can cause the most damage to the system. Once we protect against these actions (and all possible other scenarios), we are sure to protect the system from any attacker that takes over the entities that we consider to be Byzantine.

Let's now place the communication channel in this Byzantine failure model.
HTTPS, the more secure Web protocol, is built around this weaker scenario. It introduces encryption and authentication of transmitted data, which allows it to be transferred over \textit{non-trusted} communication channels.
If the data is altered by a malicious third party during transmission, the client or the web server can detect it and react accordingly.

But what if we extend the Byzantine model to other entities? For example, if a content creator publishes a website on a web hosting service, that platform is technically able to delete or alter any file of that website, effectively sending to clients different information than the one intended by the author, and the clients would not notice this difference. HTTPS would not be able to protect against this attack, since it only protects the communication channel and not the website files themselves, which are encrypted by the web server only when transmitting them to a client: to do this, the web server has to access the files directly, and can therefore do whatever possible action to them, including editing and deleting them. Not only that: the web server is also able to send whatever content it desires (or refuse to serve content at all), since the client has no way of verifying that the content is as intended by its author.

What would we achieve if we protect the data itself from Byzantine attacks? If the data can be distributed by a non-trusted entity, then we don't need to rely on some specific and trusted system: \textit{anyone} can distribute it.

This effectively enables a \textbf{distributed Web}, where content is shared by a network of web servers that anyone can provide. We don't need to trust some hosting service provider with the website data, we can distribute our data to any of them and we will be sure that the information is going to be transmitted safely without modifications.

Let's see how this scenario can resolve the three problems described in Section \ref{sec:problems}:
\begin{itemize}
	\item \textbf{Censorship}: to allow web servers to expose Byzantine behavior means that any one of them can refuse to serve content. But note that we can't assume that every single web server  fails with Byzantine consequences, just like we don't assume that in the current Web every single  server will crash when contacted. We have to guarantee that some percentage of the servers will behave properly, without failing.\\
	This is why we need a \textbf{network} of web servers and not just one: that one server not only is exposed to all of the attacks mentioned in Section \ref{sec:problems}, but can also fail by refusing to serve content; if, instead of one, we have multiple web server (of which we are guaranteed that a percentage behaves properly), we know that if some web server fails (either on its own or by means of an attack) the rest of the network can still operate.

	\item \textbf{Trust}: by distrusting the web servers, we have made impossible for them to successfully send data different from what the content creator intended. Clients have a way to detect information manipulation and can discard such manipulated data.

	\item \textbf{Privacy} and handling of personal information: in a distributed environment, we no longer have websites that can privately store data about their users. This is because every website now is hosted on multiple servers: if they want to provide some form of consistency, they have to share or synchronize that data in some way. This suggests that the data is no longer private, but we cannot have public personal data available to multiple servers, especially when some of them have malicious intents.\\
	Among the different possibilities, we mention two options here: either the clients (and the clients only) store the private data, so that it is not shared in the network, or the data is uploaded in the network but in an encrypted way, so that only its owner can access it. We will see how different projects tackle this problem, and whether they solve it.
\end{itemize}

Many projects claim to implement the distributed Web. Every one of them provides a specific strength or focuses on a particular issue instead of building a complete, all-round system.
Unfortunately, such protocols are not well-known to the general public, and consequently are not widespread enough to be either standardized or integrated into modern state-of-the-art browsers.
But most importantly, creating such a protocol is very difficult.

Keep in mind that many of these projects do not define the model around which their platform is built. They mostly define the possible attacks and the related countermeasures they adopt. We will try to infer a model of the environment whenever we consider appropriate to do so, while trying stay as close to the project specifications as possible.

%\section{Challenges of creating a distributed Web}
%todo - But how exactly is it difficult to build the distributed Web? 

\chapter{Decentralizing Storage}

\section{BitTorrent}\label{proj:bittorrent}

\subsection{DHT}\label{tech:dht}

\section{IPFS}\label{proj:ipfs}

\section{Ethereum Swarm}\label{proj:swarm}

\section{Filecoin}\label{proj:filecoin}

\section{Analysis}


\chapter{Decentralizing Naming}

\section{Namecoin}\label{proj:namecoin}

\section{Ethereum Name Service}\label{proj:ens}

\section{Analysis}


\chapter{Distributed Web Projects}

\section{ZeroNet}\label{proj:zeronet}

\section{Blockstack}\label{proj:blockstack}

\section{Analysis}


\chapter{Upcoming projects}

\section{Substratum}

\section{Hashgraph}


\chapter{Conclusions}

%As you can easily see from the above listing \citet{bbggs:iet07}
%define something weird based on the BPEL specification
%\citep{bpelspec}.
%\nocite{*}

\appendix 

\chapter{The Domain Name System - A brief overview}\label{appx:dns}
The Domain Name System is a decentralized naming system for devices connected to a network (including the Internet), currently defined with RFC 1034\cite{rfc:1034} and RFC 1035\cite{rfc:1035} and updated with successive RFCs throughout the years.

The DNS defines three components:
\begin{itemize}
	\item The \emph{domain name space} is a tree data structure, where nodes are identified by \emph{labels}: labels compose the domain names in a hierarchical way, by concatenation of labels separated by dots. For example, for the domain ``\texttt{www.example.com}'', ``\texttt{example.com}'' is a child of ``\texttt{com}'' and ``\texttt{www.example.com}'' is a child of ``\texttt{example.com}''.
	\item \emph{Name servers} are programs which store information about a subset of the domain space and references to other name servers which have information about the rest of the tree. Name servers have \emph{authority} over the parts of the tree of which they have complete information.
	\item \emph{Resolvers} are programs which receive queries from clients and respond with information extracted from the name servers. Resolvers only need to know directly just one name server to complete all possible queries: if that name server does not contain the requested information, the resolver uses its references to reach other name servers.
\end{itemize}
The domain name space has one root node, labeled with an empty string. Childs of this node are called \emph{top-level domains}, among which we can find ``\texttt{.com}'' and ``\texttt{.org}'', and two lettered \emph{country codes}, such as ``\texttt{.ch}'' and ``\texttt{.it}''. Currently, there are about one thousand different top-level domains\cite{website:tldlist}.

When resolving a hostname, resolvers query the root name server with the whole domain. The root name server usually replies with the address of the name server which has authority over the top-level domain of the hostname, but it also has facility to reply with the address of the actual server associated with the whole hostname. If the query has not been completed, the query is repeated with the correspondent top-level domain name server, and so on.

To reduce traffic towards the root name servers (and all other name servers), DNS resolvers implement a \emph{caching} system: results from name servers are stored for reuse, together with a time-to-live value specified from the name servers themselves. 

\backmatter

\chapter{Glossary}
\begin{itemize}
	\item TCP
	\item ISP
	
\end{itemize}

%\bibliographystyle{alpha}
%\bibliographystyle{dcu}
\bibliographystyle{plainnat} %TODO - figure out how to work this
\bibliography{webrbiblio}

%\cleardoublepage
%\theindex %optional, use only if you have an index, must use
	  %\makeindex in the preamble
%\lipsum

\end{document}
